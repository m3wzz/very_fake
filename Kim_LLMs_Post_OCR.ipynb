{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3wzz/very_fake/blob/main/Kim_LLMs_Post_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Large Language Models and Post-OCR-Correction\n",
        "\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8cmVjdCB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIGZpbGw9IiNGRkZGRkYiLz4KICA8Y2lyY2xlIGN4PSIxMCIgY3k9IjEwIiByPSI5IiBmaWxsPSIjQThDRTNDIi8+CiAgPHRleHQgeD0iMTAiIHk9IjE1IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTEiIGZvbnQtd2VpZ2h0PSJib2xkIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjRkZGRkZGIj5pRDwvdGV4dD4KPC9zdmc+)](https://orcid.org/0000-0002-1031-2759)\n",
        "\n",
        "Post-OCR correction addresses errors introduced when optical character recognition (OCR) converts scanned images into digital text. Common errors include character substitutions (e.g., \"rn\" misread as \"m\"), deletions, and formatting issues caused by poor image quality, unusual fonts, or degraded historical documents. Correction techniques range from dictionary-based validation and language models that use context to identify mistakes, to modern machine learning approaches—particularly large language models (LLMs)—that can intelligently reconstruct intended meaning from garbled text. This correction step is essential for downstream applications like text mining, digital archiving, and information retrieval, as even small error rates can significantly impact analysis quality and user experience."
      ],
      "metadata": {
        "id": "2-vau_f2uMTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/soberbichler/NLP-Course4Humanities_2025.github.io.git"
      ],
      "metadata": {
        "id": "NtzhAxvnvDgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('/content/NLP-Course4Humanities_2025.github.io/datasets/lügenpresse_dataset (1).xlsx')\n",
        "\n",
        "# Now you can work with the DataFrame 'df'\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eSiWoUIcycg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[:4]"
      ],
      "metadata": {
        "id": "jVer-TDMzeD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w08KLXp_uCLc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "# API Config\n",
        "api_url = \"https://ki-chat.uni-mainz.de/api\"\n",
        "api_key = userdata.get('UNI-MAINZ')\n",
        "\n",
        "def call_mainz_api(system_prompt, user_prompt, temperature=0.0, max_tokens=20000):\n",
        "    \"\"\"\n",
        "    Call University of Mainz API with system and user prompts.\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        \"model\": \"Qwen3 235B VL\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{api_url}/chat/completions\",\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"API Error: HTTP {response.status_code} - {response.text}\")\n",
        "\n",
        "    result = response.json()\n",
        "    return result['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "# Process the DataFrame\n",
        "all_articles = []\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        # Make API call\n",
        "        content = call_mainz_api(\n",
        "            system_prompt=\"\"\"You are an expert in OCR-Post-Correction\"\"\",\n",
        "            user_prompt=f\"\"\"Please correct OCR error in the newspaper texts. If a word is not readable, add a ?. Only correct if certain\n",
        "Text to analyze:\n",
        "{row['context_small']}\"\"\",\n",
        "            temperature=0.0,\n",
        "            max_tokens=20000\n",
        "        )\n",
        "\n",
        "        # Process articles\n",
        "        if content and \"Keine Artikel mit dem angegebenen Thema gefunden.\" not in content:\n",
        "            new_row = row.to_dict()\n",
        "            new_row['article_corrected'] = content.strip()\n",
        "            all_articles.append(new_row)\n",
        "\n",
        "        print(f\"Processed row {index + 1}/{len(df)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {index}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Create final DataFrame\n",
        "result_2_df = pd.DataFrame(all_articles)\n",
        "\n",
        "# Save to Excel\n",
        "result_2_df.to_excel('test_2.xlsx', index=False)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nProcessed {len(result_2_df)} articles successfully\")\n",
        "print(result_2_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_2_df['article_corrected'][0]"
      ],
      "metadata": {
        "id": "DIkqWG_G0vHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_2_df.to_excel('test.xlsx')"
      ],
      "metadata": {
        "id": "LPYNWzKg1hjS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}