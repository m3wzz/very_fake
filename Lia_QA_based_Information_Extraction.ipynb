{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3wzz/very_fake/blob/main/Lia_QA_based_Information_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbB9AwCpIwIe"
      },
      "source": [
        "# QA-based Information Extraction\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8cmVjdCB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIGZpbGw9IiNGRkZGRkYiLz4KICA8Y2lyY2xlIGN4PSIxMCIgY3k9IjEwIiByPSI5IiBmaWxsPSIjQThDRTNDIi8+CiAgPHRleHQgeD0iMTAiIHk9IjE1IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTEiIGZvbnQtd2VpZ2h0PSJib2xkIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjRkZGRkZGIj5pRDwvdGV4dD4KPC9zdmc+)](https://orcid.org/0000-0002-1031-2759)\n",
        "\n",
        "\n",
        "QA-based Entity Extraction transforms entity recognition into a question-answering task. Instead of directly labeling words in text as entities, it asks specific questions like \"What companies are mentioned?\" or \"Who are the people in this text?\" The model then responds by extracting the relevant entities from the text as answers to these questions. This approach makes entity extraction more flexible and intuitive, as new entity types can be added simply by asking new questions, though it may be more computationally intensive than traditional sequence labeling methods.\n",
        "\n",
        "###NuExtract Lannguage Model\n",
        "\n",
        "For the NE extraction, we use the NuExtract model v2. NuExtract is trained on a private high-quality dataset for structured information extraction. It supports long documents and several languages (English, French, Spanish, German, Portuguese, and Italian)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUT-wadXyZC"
      },
      "source": [
        "## Importing the Dataset\n",
        "\n",
        "We import a dataset that contains single articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Wpvvub7IxW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/soberbichler/NLP-Course4Humanities_2025.github.io.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7ZY-SsI7LyY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('/content/NLP-Course4Humanities_2025.github.io/datasets/lügenpresse_context_window.xlsx')\n",
        "\n",
        "# Now you can work with the DataFrame 'df'\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['context_small'][0]"
      ],
      "metadata": {
        "id": "8Plwon0b_X9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUDhZrrjZ27N"
      },
      "source": [
        "##Defining a Template for Information Extraction\n",
        "\n",
        "As an example, we extract information from earthquake reportings. In doing so, we want the model to distinguish between earthquake locations, dateline locations, extract the date of the earthquake, the magnitutes, the persons involved as well as causalities, damage and rescue effort of the earthquake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m62f_xoGZ56j"
      },
      "outputs": [],
      "source": [
        "# Define a template for Lügenpresse information extraction\n",
        "import json\n",
        "lügenpresse_template = json.dumps({\n",
        "    \"Lügenpresse\": {\n",
        "        \"Places_Of_Origin_Of_The_Lie\": \"string\",\n",
        "        \"Press_Mentioned_in_Relation_To_Lying_Press\": \"string\",\n",
        "        \"What_They_Lied_About\": \"string\",\n",
        "        \"Context\": \"string\"\n",
        "    }\n",
        "}, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Model\n",
        "\n",
        "The code below extracts the named entities using the extraction template. The model output is per default a json format. We add the extracted entities to our dataframe that will be saved as excel file."
      ],
      "metadata": {
        "id": "r-cCyVHIf5Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[:10]"
      ],
      "metadata": {
        "id": "rKfM6wZO-8Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen-vl-utils\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# Suppress transformer warnings\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "def process_all_vision_info(messages, examples=None):\n",
        "    \"\"\"Process vision information - returns None for text-only inputs\"\"\"\n",
        "    from qwen_vl_utils import process_vision_info\n",
        "\n",
        "    is_batch = messages and isinstance(messages[0], list)\n",
        "    messages_batch = messages if is_batch else [messages]\n",
        "\n",
        "    all_images = []\n",
        "    for message_group in messages_batch:\n",
        "        input_message_images = process_vision_info(message_group)[0] or []\n",
        "        all_images.extend(input_message_images)\n",
        "\n",
        "    return all_images if all_images else None\n",
        "\n",
        "def predict_NuExtract(model, processor, texts, template):\n",
        "    \"\"\"Extract structured information using NuExtract 2.0\"\"\"\n",
        "    outputs = []\n",
        "\n",
        "    for text in texts:\n",
        "        # Prepare messages\n",
        "        messages = [{\"role\": \"user\", \"content\": text}]\n",
        "\n",
        "        # Apply chat template\n",
        "        formatted_text = processor.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            template=template,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        # Process inputs\n",
        "        image_inputs = process_all_vision_info(messages)\n",
        "        inputs = processor(\n",
        "            text=[formatted_text],\n",
        "            images=image_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                max_new_tokens=2048,\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        output_text = processor.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "        outputs.extend(output_text)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# Load NuExtract 2.0 model\n",
        "model_name = \"numind/NuExtract-2.0-4B\"\n",
        "device = \"cuda\"\n",
        "\n",
        "print(\"Loading NuExtract 2.0 model...\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='left',\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "# Filter texts\n",
        "df['context_small'] = df['context_small'].astype(str)\n",
        "valid_texts = df['context_small'][df['context_small'].str.strip() != '']\n",
        "\n",
        "print(f\"Processing {len(valid_texts)} texts...\")\n",
        "\n",
        "# Extract information\n",
        "all_predictions = []\n",
        "\n",
        "for i in tqdm(range(len(valid_texts)), desc=\"Extracting\"):\n",
        "    text = valid_texts.iloc[i]\n",
        "    try:\n",
        "        prediction = predict_NuExtract(model, processor, [text], lügenpresse_template)\n",
        "        all_predictions.extend(prediction)\n",
        "    except Exception as e:\n",
        "        print(f\"Error on text {i}: {e}\")\n",
        "        all_predictions.append(\"{}\")\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Parse predictions\n",
        "df['lügenpresse_extraction'] = pd.Series([None] * len(df))\n",
        "df.loc[valid_texts.index, 'lügenpresse_extraction'] = all_predictions\n",
        "\n",
        "# Flatten JSON\n",
        "def parse_lügenpresse_info(extraction):\n",
        "    try:\n",
        "        parsed = json.loads(extraction)\n",
        "        return parsed.get('Lügenpresse', {})\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "df['places_of_origin_of_the_lie'] = df['lügenpresse_extraction'].apply(lambda x: parse_lügenpresse_info(x).get('Places_Of_Origin_Of_The_Lie', ''))\n",
        "df['press_mentioned_in_relation_to_lying_press'] = df['lügenpresse_extraction'].apply(lambda x: parse_lügenpresse_info(x).get('Press_Mentioned_in_Relation_To_Lying_Press', ''))\n",
        "df['what_they_lied_about'] = df['lügenpresse_extraction'].apply(lambda x: parse_lügenpresse_info(x).get('What_They_Lied_About', ''))\n",
        "df['context_extraction'] = df['lügenpresse_extraction'].apply(lambda x: parse_lügenpresse_info(x).get('Context', ''))\n",
        "\n",
        "# Save results\n",
        "df.to_excel('lügenpresse_extractions.xlsx', index=False)\n",
        "\n",
        "print(\"Done!\")\n",
        "df"
      ],
      "metadata": {
        "id": "YvzUxCBqeulS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('results.xlsx')"
      ],
      "metadata": {
        "id": "Y2fEbj0v_xSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Example - Creating a Map with Earthquake Locations\n",
        "\n",
        "\n",
        "We first use the geopy library to process geographic locations and add their corresponding coordinates (latitude and longitude) to a pandas DataFrame. It includes a GeocodingService class that interfaces with the Nominatim geocoding API, implementing rate-limiting, retries with exponential backoff, and error handling to ensure robust geocoding.\n",
        "\n",
        "We further use the folium library to create an interactive map with markers for locations provided in a pandas DataFrame. Finally, the map is created and displayed, providing a visual representation of the geographic data.\n",
        "\n",
        "Please not that when using extracted named entities for further analysis, they need to be controlled and verified by a human reader since the model most likely has made some mistakes."
      ],
      "metadata": {
        "id": "8B-_hff0g4IO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygivtCLFk4KD"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "import pandas as pd\n",
        "import time\n",
        "from typing import List, Tuple, Optional\n",
        "import random\n",
        "\n",
        "class GeocodingService:\n",
        "    def __init__(self, user_agent: str = None, timeout: int = 10, rate_limit: float = 1.1):\n",
        "        \"\"\"\n",
        "        Initialize the geocoding service with proper configuration.\n",
        "\n",
        "        Args:\n",
        "            user_agent: Custom user agent string (default: generated)\n",
        "            timeout: Timeout for requests in seconds\n",
        "            rate_limit: Time to wait between requests in seconds\n",
        "        \"\"\"\n",
        "        if user_agent is None:\n",
        "            user_agent = f\"python_geocoding_script_{random.randint(1000, 9999)}\"\n",
        "\n",
        "        self.geolocator = Nominatim(\n",
        "            user_agent=user_agent,\n",
        "            timeout=timeout\n",
        "        )\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Implement rate limiting between requests\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request = time.time()\n",
        "\n",
        "    def geocode_location(self, location: str, max_retries: int = 3) -> Optional[Tuple[float, float]]:\n",
        "        \"\"\"\n",
        "        Geocode a single location with retries.\n",
        "\n",
        "        Args:\n",
        "            location: Location string to geocode\n",
        "            max_retries: Maximum number of retry attempts\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (latitude, longitude) or None if geocoding fails\n",
        "        \"\"\"\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                self._rate_limit_wait()\n",
        "                location_data = self.geolocator.geocode(location)\n",
        "                if location_data:\n",
        "                    return (location_data.latitude, location_data.longitude)\n",
        "                return None\n",
        "            except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"Failed to geocode '{location}' after {max_retries} attempts: {e}\")\n",
        "                    return None\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "            except Exception as e:\n",
        "                print(f\"Error geocoding '{location}': {e}\")\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def process_locations(self, locations: str) -> List[Optional[Tuple[float, float]]]:\n",
        "        \"\"\"\n",
        "        Process a comma-separated string of locations.\n",
        "\n",
        "        Args:\n",
        "            locations: Comma-separated string of location names\n",
        "\n",
        "        Returns:\n",
        "            List of coordinate tuples or None for failed geocoding\n",
        "        \"\"\"\n",
        "        if pd.isna(locations) or not locations:\n",
        "            return []\n",
        "\n",
        "        location_list = [loc.strip() for loc in locations.split(',')]\n",
        "        return [self.geocode_location(loc) for loc in location_list]\n",
        "\n",
        "def geolocate_places(df: pd.DataFrame,\n",
        "                    places_column: str = 'locations',\n",
        "                    user_agent: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add coordinates to a DataFrame based on location names.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        places_column: Name of the column containing comma-separated location strings\n",
        "        user_agent: Custom user agent string\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added 'coordinates' column\n",
        "    \"\"\"\n",
        "    geocoder = GeocodingService(user_agent=user_agent)\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # Process locations\n",
        "    result_df['coordinates'] = result_df[places_column].apply(geocoder.process_locations)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Main execution for Lügenpresse analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # Apply geocoding to the Lügenpresse DataFrame\n",
        "    df_with_coords = geolocate_places(\n",
        "        df,\n",
        "        places_column='places_of_origin_of_the_lie',\n",
        "        user_agent='lügenpresse_geocoding_service_v1.0'\n",
        "    )\n",
        "\n",
        "    # Update the original DataFrame with the new coordinates\n",
        "    df['coordinates'] = df_with_coords['coordinates']\n",
        "\n",
        "    # Display the results\n",
        "    print(\"\\nSample of geocoded locations:\")\n",
        "    print(df[['places_of_origin_of_the_lie', 'coordinates']].head())\n",
        "\n",
        "    # Optional: Display some statistics\n",
        "    total_rows = len(df)\n",
        "    successful_geocodes = df['coordinates'].apply(lambda x: len([c for c in x if c is not None])).sum()\n",
        "    failed_geocodes = df['coordinates'].apply(lambda x: len([c for c in x if c is None])).sum()\n",
        "\n",
        "    print(f\"\\nGeocoding Statistics:\")\n",
        "    print(f\"Total rows processed: {total_rows}\")\n",
        "    print(f\"Successfully geocoded: {successful_geocodes}\")\n",
        "    print(f\"Failed to geocode: {failed_geocodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbsClLRclZHY"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium import plugins\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional\n",
        "from IPython.display import display\n",
        "\n",
        "def create_location_map(df: pd.DataFrame,\n",
        "                       coordinates_col: str = 'coordinates',\n",
        "                       places_col: str = 'places_of_origin_of_the_lie',\n",
        "                       title_col: Optional[str] = None) -> folium.Map:\n",
        "    \"\"\"\n",
        "    Create an interactive map with individual markers for all Lügenpresse locations.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing coordinates and location names\n",
        "        coordinates_col: Name of column containing coordinates\n",
        "        places_col: Name of column containing location names\n",
        "        title_col: Optional column name for additional marker information\n",
        "\n",
        "    Returns:\n",
        "        folium.Map object with all locations marked individually\n",
        "    \"\"\"\n",
        "    # Initialize the map centered on Europe (where most references likely are)\n",
        "    m = folium.Map(location=[50, 10], zoom_start=4)\n",
        "\n",
        "    # Keep track of all valid coordinates for setting bounds\n",
        "    all_coords = []\n",
        "\n",
        "    # Process each row in the DataFrame\n",
        "    for idx, row in df.iterrows():\n",
        "        coordinates = row[coordinates_col]\n",
        "        places = row[places_col].split(',') if pd.notna(row[places_col]) else []\n",
        "        title = row[title_col] if title_col and pd.notna(row[title_col]) else None\n",
        "\n",
        "        # Skip if no coordinates\n",
        "        if not coordinates:\n",
        "            continue\n",
        "\n",
        "        # Add individual markers for each location\n",
        "        for i, (coord, place) in enumerate(zip(coordinates, places)):\n",
        "            if coord is not None:  # Skip None coordinates\n",
        "                lat, lon = coord\n",
        "                place_name = place.strip()\n",
        "\n",
        "                # Create popup content\n",
        "                popup_content = f\"<b>{place_name}</b>\"\n",
        "                if title:\n",
        "                    popup_content += f\"<br>{title}\"\n",
        "\n",
        "                # Add marker directly to the map\n",
        "                folium.Marker(\n",
        "                    location=[lat, lon],\n",
        "                    popup=folium.Popup(popup_content, max_width=300),\n",
        "                    tooltip=place_name,\n",
        "                    icon=folium.Icon(color='red', icon='info-sign')\n",
        "                ).add_to(m)\n",
        "\n",
        "                all_coords.append([lat, lon])\n",
        "\n",
        "    # If we have coordinates, fit the map bounds to include all points\n",
        "    if all_coords:\n",
        "        m.fit_bounds(all_coords)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Create and display the map\n",
        "map_obj = create_location_map(df)\n",
        "display(map_obj)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}